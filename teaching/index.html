
<!DOCTYPE html>
<html lang="en-us">

<head>
    <meta charset="utf-8">
    <meta name="author" content="Shijun Zhang"> 
<!--     <meta name="viewport" content="width=device-width, initial-scale=1.0"> -->
    <meta property="og:site_name" content="Shijun Zhang"> 
    <meta property="og:title" content="Shijun Zhang">

    <meta name="description" content="The personal website of Shijun Zhang (张仕俊)"> 
    <meta property="og:description" content="The personal website of Shijun Zhang (张仕俊)"> 

    <meta name="keywords" content="Shijun Zhang, 张仕俊, ShijunZhang, Zhang Shijun, ZhangShijun, Zhang, Shijun, 张, 仕俊, 俊" />

    <meta name="robots" content="index, follow" />
    <meta name="google" content="index, follow" />
    <meta name="googlebot" content="index, follow" />
    <meta name="verify" content="index, follow" />

<!--     <meta name="theme-color" content="#EF525B"> -->
    <meta property="og:url" content="/"> 
    <meta property="og:locale" content="en-us">
    <link rel="alternate" hreflang="en-us" href="/">
    <link rel="canonical" href="/">  

<title>Shijun Zhang 张仕俊</title>

<link rel="stylesheet" href="../file/cssAndJs/style.css"> 

<style type="text/css">
        .coursesContainer {
        display: flex;
        height: auto;
        width:auto;
        justify-content: space-between;
        align-items: flex-start;
        flex-flow: row wrap;
        margin: 0px;
        padding: 0px;
        border: 0px dashed red;
        margin-top: 0.3em;
        margin-bottom: 0.3em;
    }

    .coursesContainer span:first-child {
        width:auto;
        height:auto;
        padding: 0px;
        text-align: left;
        margin:0px;
    }

    .coursesContainer span:last-child {
        width:auto;
        height:auto;
        padding: 0px;
        text-align: right;
        margin-right:0px;
        margin-left: auto;
    }


    .coursesItemSkip {
        padding-top:0.9em;
    }

    .coursesName {
        font-size:100%;
/*        font-style: italic;
        font-family:serif;*/
        font-weight: bold;
    }


    h2 {
        padding-top:0.7em;
        padding-bottom: 0.1em;
        margin-bottom: 0.69em;
    }
    h2.first {
        padding-top: 0.58em;
        padding-bottom: 0.0em;
    }

    ul.myInterests {
      padding-top: 0.1em;
      margin-top: 0em;
    }
    .myInterests li {
      padding-top: 0.3em;
      padding-bottom: 0.44em;
    }
</style>

<style id="styleInPublicJs"></style>
<style id="styleInJs"></style>

</head>



<body class="normalFont">  <noscript> <p style="margin: 40px;font-size:1.5em;line-height: 1.5;"> You are visiting the personal website of Shijun Zhang (张仕俊) at
    <a href="https://shijunzhang.top">https://shijunzhang.top</a>.<br><br>
For full functionality of this site, it is necessary to enable JavaScript.</p></noscript>
<div style="display:none;" id="bodyDiv">
<!-- div next to body -->



<!-- The background will be added later via js. -->
<!-- <canvas id="background"></canvas> -->


<!-- add nav in js -->
<div id='navBarHTML'></div>


<section id="preSection"></section>



<section class="pageContainer normalFont">



<!-- <h1> Interests </h1> -->


<!-- <h1> Research </h1> -->

<br><br>

My teaching philosophy is anchored in the belief that each student possesses unique
and unrealized potential. As an educator, I consider it my duty to create a dynamic and
stimulating learning environment where students can discover and leverage their academic
and personal potential. In this context, my role shifts to that of a facilitator and mentor,
nurturing creative thinking and prompting students to explore their natural inquisitiveness.
It is essential to foster a classroom atmosphere where students are comfortable sharing their
thoughts and understand that no question is too small or unimportant. Additionally, I am
committed to consistently refining and tailoring my teaching strategies. This process of
ongoing enhancement is shaped by the feedback I receive from students, insights shared by
fellow educators, and various online tools, all of which play a pivotal role in my continuous
professional development. 
Refer to my <a href="./file/pdf/CV_ShijunZHANG.pdf">Curriculum Vitae</a> for the list of courses I have taught.
<!-- Below are some courses I taught. -->

<!-- <br>

<ul>

<li>
    <div  class="coursesContainer">
            <span> 
                <span class="coursesName">Mathematical Numerical Analysis</span> 
                 (Math 361S), Duke University,<br>
            Instructor, <a href="../file/teaching/syllabus2024(361S).pdf">Syllabus</a> 
            </span>
        <span> <br>Spring 2024</span>
    </div>
</li>

<li>
    <div  class="coursesContainer">
            <span> 
                <span class="coursesName">Matrices and Vectors</span> 
                 (Math 218D-2), Duke University,<br>
            Teaching assistant
            </span>
        <span> <br>Fall 2023</span>
    </div>
</li>

</ul> -->

<br><br>
<!-- 
<h2 class="first"> Research Interests </h2>
<ul class='myInterests'>
  <li>The <b>approximation</b> of deep learning: <br> How and why can deep neural networks approximate given target functions well?</li>
  <li>The <b>generalization</b> of deep learning: <br>Why do trained models work well on similar but unobserved (test) samples?</li>
</ul>

<h2> Research statement (<a href="../file/pdf/ResearchStatement_ShijunZHANG.pdf" title="Research Statement">PDF</a>) </h2>




 &emsp;&emsp;
At this point in my career, my primary interest is
in contributing to a deeper understanding of deep learning.
Deep neural networks make up the backbone of deep learning algorithms and they
 have made significant impacts on many applications in science, engineering, technology, and industries, especially for large-scale and high-dimensional
learning problems. 
<!- - Well-designed neural network architectures, efficient training algorithms,
and high-performance computing technologies have made neural-network-based methods very
successful in real applications.  - ->
The great advantages of
neural-network-based methods have been demonstrated over traditional learning methods in real applications from many empirical points of view.
Understanding the approximation power of deep neural networks theoretically has become a
key question for revealing the power of deep learning. The majority of my current research
focuses on the approximation of deep neural networks.
<br> 

<!- - Deep neural networks  have made significant impacts in many 
fields of computer science and engineering, especially for large-scale and high-dimensional learning problems. Well-designed neural network architectures, efficient training algorithms, and high-performance computing technologies have made neural-network-based methods very successful in real applications. 

<a href="../file/img/DLapplications.png"> 
  <img src="../file/img/DLapplicationsCompressed.png" class="imgCenter imgWidth" />  
</a>


Especially in supervised learning, e.g., image classification and objective detection, the great advantages of neural-network-based methods have been demonstrated over traditional learning methods. Understanding the approximation power of deep neural networks has become a key question for revealing the power of deep learning. A large number of experiments in real applications have shown the large capacity of deep network approximation from many empirical points of view, motivating much effort in establishing the theoretical foundation of deep network approximation. One of the fundamental problems is the characterization of the (optimal) approximation error of deep neural networks in terms of the network size measured in the width, the depth, the number of neurons, or the number of parameters.<br><br> - ->



<a href="../file/img/AEOEGE.pdf"> <img src="../file/img/AEOEGE.png" class="imgCenter imgWidth" id="AOGE"/>  </a>
 &emsp;&emsp;
The figure above gives an illustraction of the full error analysis of deep neural networks, which includes the approximation error (<b>AE</b>), the optimization error (<b>OE</b>), and the generalization error (<b>GE</b>). 
<!- - Currently, I am quite interested in studying the approximation error.  - ->
See  <a href="../file/pdf/Error_Analysis.pdf">this note</a> for a detailed dicussion.
Most of my papers aim to construct a network-generated function to approximate the target function well, providing a good upper bound of the approximation error.
Designing efficient algorithms to control the optimization error and analyzing the generalization error are two other separate future directions. 
In the immediate future, I would like to continue the study of the approximation error and
start analyzing the generalization error theoretically.
-->


<!-- See <a href="../file/pdf/Intro_of_AE_OE_and_GE.pdf">this note</a> for the introductions of the approximation error (<b>AE</b>), the optimization error (<b>OE</b>), and the generalization error (<b>GE</b>).  -->
<!-- <div class='addWhiteSpaceOne'></div>
<a href="../file/img/AEOEGE.png"> <img src="../file/img/AEOEGE.png" class="imgCenter" style="width:68%; " />  </a>
<div class='addWhiteSpaceOne'></div>
<span style="color:#FFFAF0;">\( \require{color}
 \def\R{{\mathbb{R}}}
    \def\bm#1{{\boldsymbol{#1}}}
   \def\tn#1{{\text{#1}}}
   \def\argmin{\mathop{\tn{arg\(\,\)min}}}
\)</span>

In supervised learning, an unknown target function $f(\boldsymbol{x})$ defined on a domain $\mathcal{X}$ is learned through its finitely many samples $\{( \boldsymbol{x}_i,f(\boldsymbol{x}_i){ )}\}_{i=1}^n$. Let $\phi(\bm{x};\bm{\theta})$ denote a function computed by  a (fully-connected) network with $\bm{\theta}$ as the set of parameters (see a network example below). 
<div class='addWhiteSpaceOne'></div>
<a href="../file/img/ReLUeg.pdf"> <img src="../file/img/ReLUeg.png" class="imgCenter" style="width:68%; " />  </a>
<div style="text-align: center;font-size: 90%;"> An example of a ReLU network with width 5 and depth 2.</div>
<div class='addWhiteSpaceOne'></div>
If neural networks are applied in supervised learning, the following optimization problem is solved to identify $\phi(\boldsymbol{x};\boldsymbol{\theta}_{\mathcal{S}})$ to infer $f(\boldsymbol{x})$ for unseen data samples $\boldsymbol{x}$, where $\boldsymbol{\theta}_{\mathcal{S}}$ is the empirical risk minimizer given by
  \begin{equation}\label{eqn:emloss}
    \boldsymbol{\theta}_{\mathcal{S}}=\argmin_{\boldsymbol{\theta}}R_{\mathcal{S}}(\boldsymbol{\theta}),
  \end{equation}
  where $R_{\mathcal{S}}(\boldsymbol{\theta})$ is the empirical risk (based on data <b>samples</b>) defined as
    \begin{equation*}
    R_{\mathcal{S}}(\boldsymbol{\theta}):=
    \frac{1}{n}\sum_{i=1}^n \ell\big( \phi(\boldsymbol{x}_i;\boldsymbol{\theta}),f(\boldsymbol{x}_i)\big) 
  \end{equation*}
  with a loss function typically taken as $\ell(y,y')=\frac{1}{2}|y-y'|^2$. <br><br>

  In fact, the expected inference error/rick (based on the data <b>distribution</b>) is defined as
  \begin{equation*}%\label{eqn:poloss}
    R_{\mathcal{D}}(\bm{\theta}):=
    \mathbb{E}_{\bm{x}\sim U(\mathcal{X})} \left[\ell( \phi(\bm{x};\bm{\theta}),f(\bm{x}))\right],
  \end{equation*}
  where the expectation is taken with an unknown data distribution $U(\mathcal{X})$ over $\mathcal{X}$. And the expected risk minimizer $\bm{\theta}_{\mathcal{D}}$ is given by
  \begin{equation*}%\label{eqn:poloss}
    \bm{\theta}_{\mathcal{D}}=\argmin_{\bm{\theta}} R_{\mathcal{D}}(\bm{\theta}).
  \end{equation*}
   We use the empirical risk $R_{\mathcal{S}}(\boldsymbol{\theta})$ instead of $R_{\mathcal{D}}(\boldsymbol{\theta})$ in real applications beacuse $f$ and $U(\mathcal{X})$ are not available.
  In the case that $U(\mathcal{X})$ is a uniform distribution on $\mathcal{X}=[0,1]^d$ and that $\ell(y,y')=\frac{1}{2}|y-y'|^2$, 
	<div class='eqFont'>-->

<!-- <div class='eqFont' style="font-size: 88%;">  
      \begin{equation*}
		R_{\mathcal{D}}(\bm{\theta})=
		\mathbb{E}_{\bm{x}\sim U(\mathcal{X})} \left[\ell( \phi(\bm{x};\bm{\theta}),f(\bm{x}))\right]=\int_{[0,1]^d}\tfrac12|\phi(\bm{x}; \bm{\theta})-f(\bm{x})|^2 d\bm{x}.
	\end{equation*} 
</div> -->

<!-- </div></div> -->


 <!--    Note that the best solution to infer $f(\bm{x})$ is $\phi(\bm{x};\bm{\theta}_{\mathcal{D}})$ with the expected risk minimizer $\bm{\theta}_{\mathcal{D}}$ given by
  \begin{equation*}%\label{eqn:poloss}
    \bm{\theta}_{\mathcal{D}}=\argmin_{\bm{\theta}} R_{\mathcal{D}}(\bm{\theta}).
  \end{equation*}
  The best possible inference error is
  $R_{\mathcal{D}}(\bm{\theta}_{\mathcal{D}})$. In real applications, $U(\mathcal{X})$ is unknown and only finitely many samples from this distribution are available. Hence, the empirical loss $R_{\mathcal{S}}(\bm{\theta})$ is minimized hoping to obtain $\phi(\bm{x};\bm{\theta}_{\mathcal{S}})$, instead of minimizing the population loss $R_{\mathcal{D}}(\bm{\theta})$ to obtain $\phi(\bm{x};\bm{\theta}_{\mathcal{D}})$. 
  In practice, a numerical optimization method to solve Equation \eqref{eqn:emloss} may result in a numerical solution (denoted as $\bm{\theta}_{\mathcal{N}}$) that may not be a global minimizer $\bm{\theta}_{\mathcal{S}}$. Therefore, the actually learned neural network to infer $f(\bm{x})$ is $\phi(\bm{x};\bm{\theta}_{\mathcal{N}})$ and the corresponding inference error is measured by $R_{\mathcal{D}}(\bm{\theta}_{\mathcal{N}})$. <br><br>
  
  By the discussion just above, it is crucial to quantify $R_{\mathcal{D}}(\bm{\theta}_{\mathcal{N}})$ to see how good the learned neural network $\phi(\bm{x};\bm{\theta}_{\mathcal{N}})$ is, since $R_{\mathcal{D}}(\bm{\theta}_{\mathcal{N}})$ is the expected inference error over all possible data samples. Note that 
  <div class='eqFont' style="font-size: 88%;"> 
   <div style='font-size: 79%;'>  
  \begin{align}
    R_{\mathcal{D}}(\bm{\theta}_{\mathcal{N}})
    &=\underbrace{[R_{\mathcal{D}}(\bm{\theta}_{\mathcal{N}})-R_{\mathcal{S}}(\bm{\theta}_{\mathcal{N}})]}_{\tn{GE}}
    +\underbrace{[R_{\mathcal{S}}(\bm{\theta}_{\mathcal{N}})-R_{\mathcal{S}}(\bm{\theta}_{\mathcal{S}})]}_{\tn{OE}}   +\underbrace{[R_{\mathcal{S}}(\bm{\theta}_{\mathcal{S}})-R_{\mathcal{S}}(\bm{\theta}_{\mathcal{D}})]}_{\tn{$\le 0$ by Equation \eqref{eqn:emloss}}} +\underbrace{[R_{\mathcal{S}}(\bm{\theta}_{\mathcal{D}})-R_{\mathcal{D}}(\bm{\theta}_{\mathcal{D}})]}_{\tn{GE}}
    +\underbrace{R_{\mathcal{D}}(\bm{\theta}_{\mathcal{D}})}_{\tn{AE}}
    \nonumber \\
      \label{eqn:gen} &\le{ \underbrace{R_{\mathcal{D}}(\bm{\theta}_{\mathcal{D}})}_{\color{blue}\tn{Approximation error (AE)}} \ +\  \underbrace{[R_{\mathcal{S}}(\bm{\theta}_{\mathcal{N}})-R_{\mathcal{S}}(\bm{\theta}_{\mathcal{S}})]}_{\color{blue}\tn{Optimization error (OE)}}\  + \  \underbrace{[R_{\mathcal{D}}(\bm{\theta}_{\mathcal{N}})-R_{\mathcal{S}}(\bm{\theta}_{\mathcal{N}})]
      +[R_{\mathcal{S}}(\bm{\theta}_{\mathcal{D}})-R_{\mathcal{D}}(\bm{\theta}_{\mathcal{D}})]}_{\color{blue}\tn{Generalization error (GE)}}},
  \end{align}
</div>
 </div></div> 
where the inequality comes from the fact that $[R_{\mathcal{S}}(\bm{\theta}_{\mathcal{S}})-R_{\mathcal{S}}(\bm{\theta}_{\mathcal{D}})]\leq 0$ since $\bm{\theta}_{\mathcal{S}}$ is a global minimizer of $R_{\mathcal{S}}(\bm{\theta})$. <br><br>
Constructive approximation provides an upper bound of $R_{\mathcal{D}}(\bm{\theta}_{\mathcal{D}})$ in terms of the network size, e.g., in terms of the network width and depth, or in terms of the number of parameters.  The second term of Equation \eqref{eqn:gen} is bounded by the optimization error of the numerical algorithm applied to solve the empirical loss minimization problem in Equation \eqref{eqn:emloss}. The study of the bounds for the third and fourth terms is referred to as the generalization error analysis of neural networks. 
It is an interesting topic to develop deep learning models and algorithms to minimize optimization and generalization errors.
<br><br> -->
 
	
<!-- 	One of  the key targets in  the area of deep learning is to develop algorithms to reduce  $R_{\mathcal{D}}{({\bm{\theta}}_{\mathcal{N}})}$.
Currently, we focus on providing an upper bound of the approximation  error $R_{\mathcal{D}}(\bm{\theta}_{\mathcal{D}})$ for smooth functions, which is crucial to estimate an upper bound of $R_{\mathcal{D}}{({\bm{\theta}}_{\mathcal{N}})}$.   Instead of deriving an approximator to attain  the approximation error bound,  deep learning models and algorithms aim at identifying a solution $\phi(\bm{x};\bm{\theta}_{\mathcal{N}})$ reducing the generalization and optimization errors in Equation \eqref{eqn:gen}.  Solutions minimizing both generalization and optimization errors will lead to a good solution only if we also have a good upper bound estimate of $R_{\mathcal{D}}(\bm{\theta}_{\mathcal{D}})$ as shown in Equation \eqref{eqn:gen}.   Independent of whether our analysis here  leads to a good approximator, which is an interesting topic to pursue,  the  theory here does provide a key ingredient in the error analysis of deep learning models and algorithms.
 


<!-- The approximation theory of neural networks is interesting and technical. Unlike traditional tools to approximate functions, neural networks has two key benefits.
<ul>
  <li>The architecture of neural networks is easy but powerful because of compositions. For example, if we use $f_2\circ f_1$ to approximate $f$, we only need to care the values of $f_2$ on $\text{Im}f_1$, which is extremely efficient when $\text{Im}f_1$ only contains finite points. </li>
  <li>Neural networks is flexible. That is, neural networks can perform well even without the latent structures of the target functions.</li>
</ul> -->
<!-- <ol>
    <li> <b> Approximation Theory.</b> I am pretty interested in technical constructive approxiamtion problems, such as Wavelet, Neural Network.</li>
    <li> <b>Neural Network.</b> I am quite curious about the expresssive power of neural networks to approximate other functions, like continuous functions.
    And I wonder why neural networks work so well in several tasks, such as classification problems. </li>
</ol>

<div class="addWhiteSpaceOne"></div> -->


<!-- <h2> Other Interests:</h2>
<ol>
    <li> <b>Code.</b> I like several code languages, like Latex, Python.
            <ul>
            <li> <b>Latex.</b> I was astonished by Latex when I used Latex for the first time.
            To understant why Latex can typeset contents much better than word, I dive deeper into
            Latex and  learned to write &ldquo;class&rdquo; (.cls) and &ldquo;style&rdquo; (.sty) by myself.</li>

            <li> <b>Python.</b> I learned Python since the first year of my Ph.D. program. Attracted by its rich extensive libraries, 
            I use it to do lots of interesting things: making dynamic images, dealing with pdf files, implementing all kinds of deep learning algorithms, etc.   </li>
            </ul>
    </li><br />
    <li> <b>Dynamic Images.</b> </li>  Dynamic images is interesting and I prefer to make mathematical dynamic images by myself.
    See <a href="../more/"> <b> More</b></a> for examples.
</ol> -->



</section>



<section id="postSection"></section>


<!-- footer section -->
<section>
<footer id="footer"></footer>
</section>



<script type="text/javascript" src="../file/cssAndJs/script.js"></script>
<!-- <script type="text/javascript" src="./interest.js"></script> -->
<script type="text/javascript"> 
    if (phoneOrPc) {
      var pageWidth=0.875*w;
      var imgWidth=".imgWidth {width:88%;padding-top:1.52em;padding-bottom:1.52em;}";
    } else {
      var pageWidth=0.49*w;
      imgWidth=".imgWidth {width:68%;padding-top:1.73em;padding-bottom:1.73em;}";
    }

    var insertInterestCss=" .pageContainer {display:block;margin:0 auto;padding-left:3%;"+
    " padding-right:3%;width:"+pageWidth+"px;}"+ imgWidth;
    document.getElementById("styleInJs").innerHTML=insertInterestCss;

    if (phoneOrPc) {
        document.getElementById("AOGE").style.width="99%";
      }else{
        document.getElementById("AOGE").style.width="85%";
      }
        // pre and post section
    var preSection=document.getElementById("preSection");
    var postSection=document.getElementById("postSection");

    if (phoneOrPc){
        preSection.style.height="1.362em"; /*0.0236*h +"px";*/
        postSection.style.height="3.92em"; /*0.10*h +"px";*/
    }else{
        preSection.style.height="11.6em"; /*0.034*h +"px";*/
        postSection.style.height="5em"; /*0.132*h +"px";*/
    }
</script>


<!-- add background on canvas -->
<!-- <script type="text/javascript" src="../file/background/bg.js"></script> -->



<!-- div next to body -->
</div>
</body>



</html>


<!-- 页面 A 跳到 B 页面的特定位置
<a href="../more/#content">点击转到内容</a>
<a href="https://www.baidu.com/#content">点击转到内容</a>
<a id="content">内容部分</a> -->