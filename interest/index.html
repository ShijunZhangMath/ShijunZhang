
<!DOCTYPE html>
<html lang="en-us">

<head>
  <meta charset="utf-8">
  <meta name="author" content="Shijun Zhang"> 
  <meta property="og:site_name" content="Shijun Zhang"> 
  <meta property="og:title" content="Shijun Zhang">

  <meta name="description" content="The homepage of Shijun Zhang (张仕俊)"> 
  <meta property="og:description" content="The homepage of Shijun Zhang (张仕俊)"> 

  <meta name="keywords" content="Shijun Zhang, 张仕俊, ShijunZhang, Zhang Shijun, ZhangShijun, Zhang, Shijun" />

  <meta name="robots" content="index, follow" />
  <meta name="google" content="index, follow" />
  <meta name="googlebot" content="index, follow" />
  <meta name="verify" content="index, follow" />

  <meta name="theme-color" content="#EF525B">
  <meta property="og:url" content="/"> 
  <meta property="og:locale" content="en-us">
  <link rel="alternate" hreflang="en-us" href="/">
  <link rel="canonical" href="/">  

  <!--   <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png"> -->
  <!-- The icon is changed in script.js -->

 <title>Shijun Zhang</title>

<link rel="stylesheet" href="../file/cssAndJs/style.css"> 
<style id="styleInJs"></style>
<link rel="stylesheet" href="../file/background/bgSetting.css"> 


<!--math formula below  from web: http://docs.mathjax.org/en/latest/configuration.html-->
<!-- <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    "HTML-CSS": { fonts: ["TeX"] }
  });
</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js">
</script> -->



<!-- <script type="text/javascript">
  MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    tags: 'all',
  },
  chtml: {
    scale: 0.92, 
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script> -->



</head>



<body>
<!-- The background will be added later via js. -->
<canvas id="background"></canvas>


<!-- add nav in js -->
<div id='navBarHTML'></div>

<!-- <nav>
  <ul class="navBarContainer navBarLinkColor" id="navBar">
    <li id="leftMostNavBarItem" class="navBarItem"></li>
    <li class="navBarItem" >
      <a href="../" > Home </a>
    </li>
    <li class="navBarItem">
      <a href="../publication/"> Publication </a>
    </li>
        <li class="navBarItem">
      <a href="../interest/"  class="activeLinkColor"> Interest </a>
    </li>
    <li class="navBarItem">
      <a href="../cv/">  Curriculum Vitae </a>
    </li>
    <li class="navBarItem">
      <a href="../more/"> More </a>
    </li>
  </ul>
</nav> -->


<section id="preSection"></section>



<section class="pageContainer normalFont">



<!-- <h1> Interests </h1> -->


<h1> Research Interests</h1>
<br>
Deep neural networks  have made significant impacts in many 
fields of computer science and engineering, especially for large-scale and high-dimensional learning problems. Well-designed neural network architectures, efficient training algorithms, and high-performance computing technologies have made neural-network-based methods very successful in real applications. 

<br><br>
<a href="../file/img/DLapplications.png"> 
  <img src="../file/img/DLapplications.png" class="imgCenter imgWidth" />  
</a>
<br><br>

Especially in supervised learning, e.g., image classification and objective detection, the great advantages of neural-network-based methods have been demonstrated over traditional learning methods. Understanding the approximation power of deep neural networks has become a key question for revealing the power of deep learning. A large number of experiments in real applications have shown the large capacity of deep network approximation from many empirical points of view, motivating much effort in establishing the theoretical foundation of deep network approximation. One of the fundamental problems is the characterization of the (optimal) approximation error of deep neural networks in terms of the network size measured in the width, the depth, the number of neurons, or the number of parameters.<br><br>

Currently, I am quite interested in studying the approximation error. 
Designing efficient optimization algorithms and analyzing the generalization bounds are two other separate future directions. 
See <a href="../file/pdf/Intro_of_AE_OE_and_GE.pdf">this note</a> for the introductions of the approximation error (<b>AE</b>), the optimization error (<b>OE</b>), and the generalization error (<b>GE</b>). 


<br><br>

<a href="../file/img/AEOEGE.png"> <img src="../file/img/AEOEGE.png" class="imgCenter imgWidth" />  </a>

<!-- <div class='addWhiteSpaceOne'></div>
<a href="../file/img/AEOEGE.png"> <img src="../file/img/AEOEGE.png" class="imgCenter" style="width:68%; " />  </a>
<div class='addWhiteSpaceOne'></div>
<span style="color:#FFFAF0;">\( \require{color}
 \def\R{{\mathbb{R}}}
    \def\bm#1{{\boldsymbol{#1}}}
   \def\tn#1{{\text{#1}}}
   \def\argmin{\mathop{\tn{arg\(\,\)min}}}
\)</span>

In supervised learning, an unknown target function $f(\boldsymbol{x})$ defined on a domain $\mathcal{X}$ is learned through its finitely many samples $\{( \boldsymbol{x}_i,f(\boldsymbol{x}_i){ )}\}_{i=1}^n$. Let $\phi(\bm{x};\bm{\theta})$ denote a function computed by  a (fully-connected) network with $\bm{\theta}$ as the set of parameters (see a network example below). 
<div class='addWhiteSpaceOne'></div>
<a href="../file/img/ReLUeg.pdf"> <img src="../file/img/ReLUeg.png" class="imgCenter" style="width:68%; " />  </a>
<div style="text-align: center;font-size: 90%;"> An example of a ReLU network with width 5 and depth 2.</div>
<div class='addWhiteSpaceOne'></div>
If neural networks are applied in supervised learning, the following optimization problem is solved to identify $\phi(\boldsymbol{x};\boldsymbol{\theta}_{\mathcal{S}})$ to infer $f(\boldsymbol{x})$ for unseen data samples $\boldsymbol{x}$, where $\boldsymbol{\theta}_{\mathcal{S}}$ is the empirical risk minimizer given by
  \begin{equation}\label{eqn:emloss}
    \boldsymbol{\theta}_{\mathcal{S}}=\argmin_{\boldsymbol{\theta}}R_{\mathcal{S}}(\boldsymbol{\theta}),
  \end{equation}
  where $R_{\mathcal{S}}(\boldsymbol{\theta})$ is the empirical risk (based on data <b>samples</b>) defined as
    \begin{equation*}
    R_{\mathcal{S}}(\boldsymbol{\theta}):=
    \frac{1}{n}\sum_{i=1}^n \ell\big( \phi(\boldsymbol{x}_i;\boldsymbol{\theta}),f(\boldsymbol{x}_i)\big) 
  \end{equation*}
  with a loss function typically taken as $\ell(y,y')=\frac{1}{2}|y-y'|^2$. <br><br>

  In fact, the expected inference error/rick (based on the data <b>distribution</b>) is defined as
  \begin{equation*}%\label{eqn:poloss}
    R_{\mathcal{D}}(\bm{\theta}):=
    \mathbb{E}_{\bm{x}\sim U(\mathcal{X})} \left[\ell( \phi(\bm{x};\bm{\theta}),f(\bm{x}))\right],
  \end{equation*}
  where the expectation is taken with an unknown data distribution $U(\mathcal{X})$ over $\mathcal{X}$. And the expected risk minimizer $\bm{\theta}_{\mathcal{D}}$ is given by
  \begin{equation*}%\label{eqn:poloss}
    \bm{\theta}_{\mathcal{D}}=\argmin_{\bm{\theta}} R_{\mathcal{D}}(\bm{\theta}).
  \end{equation*}
   We use the empirical risk $R_{\mathcal{S}}(\boldsymbol{\theta})$ instead of $R_{\mathcal{D}}(\boldsymbol{\theta})$ in real applications beacuse $f$ and $U(\mathcal{X})$ are not available.
  In the case that $U(\mathcal{X})$ is a uniform distribution on $\mathcal{X}=[0,1]^d$ and that $\ell(y,y')=\frac{1}{2}|y-y'|^2$, 
	<div class='eqFont'>-->

<!-- <div class='eqFont' style="font-size: 88%;">  
      \begin{equation*}
		R_{\mathcal{D}}(\bm{\theta})=
		\mathbb{E}_{\bm{x}\sim U(\mathcal{X})} \left[\ell( \phi(\bm{x};\bm{\theta}),f(\bm{x}))\right]=\int_{[0,1]^d}\tfrac12|\phi(\bm{x}; \bm{\theta})-f(\bm{x})|^2 d\bm{x}.
	\end{equation*} 
</div> -->

<!-- </div></div> -->


 <!--    Note that the best solution to infer $f(\bm{x})$ is $\phi(\bm{x};\bm{\theta}_{\mathcal{D}})$ with the expected risk minimizer $\bm{\theta}_{\mathcal{D}}$ given by
  \begin{equation*}%\label{eqn:poloss}
    \bm{\theta}_{\mathcal{D}}=\argmin_{\bm{\theta}} R_{\mathcal{D}}(\bm{\theta}).
  \end{equation*}
  The best possible inference error is
  $R_{\mathcal{D}}(\bm{\theta}_{\mathcal{D}})$. In real applications, $U(\mathcal{X})$ is unknown and only finitely many samples from this distribution are available. Hence, the empirical loss $R_{\mathcal{S}}(\bm{\theta})$ is minimized hoping to obtain $\phi(\bm{x};\bm{\theta}_{\mathcal{S}})$, instead of minimizing the population loss $R_{\mathcal{D}}(\bm{\theta})$ to obtain $\phi(\bm{x};\bm{\theta}_{\mathcal{D}})$. 
  In practice, a numerical optimization method to solve Equation \eqref{eqn:emloss} may result in a numerical solution (denoted as $\bm{\theta}_{\mathcal{N}}$) that may not be a global minimizer $\bm{\theta}_{\mathcal{S}}$. Therefore, the actually learned neural network to infer $f(\bm{x})$ is $\phi(\bm{x};\bm{\theta}_{\mathcal{N}})$ and the corresponding inference error is measured by $R_{\mathcal{D}}(\bm{\theta}_{\mathcal{N}})$. <br><br>
  
  By the discussion just above, it is crucial to quantify $R_{\mathcal{D}}(\bm{\theta}_{\mathcal{N}})$ to see how good the learned neural network $\phi(\bm{x};\bm{\theta}_{\mathcal{N}})$ is, since $R_{\mathcal{D}}(\bm{\theta}_{\mathcal{N}})$ is the expected inference error over all possible data samples. Note that 
  <div class='eqFont' style="font-size: 88%;"> 
   <div style='font-size: 79%;'>  
  \begin{align}
    R_{\mathcal{D}}(\bm{\theta}_{\mathcal{N}})
    &=\underbrace{[R_{\mathcal{D}}(\bm{\theta}_{\mathcal{N}})-R_{\mathcal{S}}(\bm{\theta}_{\mathcal{N}})]}_{\tn{GE}}
    +\underbrace{[R_{\mathcal{S}}(\bm{\theta}_{\mathcal{N}})-R_{\mathcal{S}}(\bm{\theta}_{\mathcal{S}})]}_{\tn{OE}}   +\underbrace{[R_{\mathcal{S}}(\bm{\theta}_{\mathcal{S}})-R_{\mathcal{S}}(\bm{\theta}_{\mathcal{D}})]}_{\tn{$\le 0$ by Equation \eqref{eqn:emloss}}} +\underbrace{[R_{\mathcal{S}}(\bm{\theta}_{\mathcal{D}})-R_{\mathcal{D}}(\bm{\theta}_{\mathcal{D}})]}_{\tn{GE}}
    +\underbrace{R_{\mathcal{D}}(\bm{\theta}_{\mathcal{D}})}_{\tn{AE}}
    \nonumber \\
      \label{eqn:gen} &\le{ \underbrace{R_{\mathcal{D}}(\bm{\theta}_{\mathcal{D}})}_{\color{blue}\tn{Approximation error (AE)}} \ +\  \underbrace{[R_{\mathcal{S}}(\bm{\theta}_{\mathcal{N}})-R_{\mathcal{S}}(\bm{\theta}_{\mathcal{S}})]}_{\color{blue}\tn{Optimization error (OE)}}\  + \  \underbrace{[R_{\mathcal{D}}(\bm{\theta}_{\mathcal{N}})-R_{\mathcal{S}}(\bm{\theta}_{\mathcal{N}})]
      +[R_{\mathcal{S}}(\bm{\theta}_{\mathcal{D}})-R_{\mathcal{D}}(\bm{\theta}_{\mathcal{D}})]}_{\color{blue}\tn{Generalization error (GE)}}},
  \end{align}
</div>
 </div></div> 
where the inequality comes from the fact that $[R_{\mathcal{S}}(\bm{\theta}_{\mathcal{S}})-R_{\mathcal{S}}(\bm{\theta}_{\mathcal{D}})]\leq 0$ since $\bm{\theta}_{\mathcal{S}}$ is a global minimizer of $R_{\mathcal{S}}(\bm{\theta})$. <br><br>
Constructive approximation provides an upper bound of $R_{\mathcal{D}}(\bm{\theta}_{\mathcal{D}})$ in terms of the network size, e.g., in terms of the network width and depth, or in terms of the number of parameters.  The second term of Equation \eqref{eqn:gen} is bounded by the optimization error of the numerical algorithm applied to solve the empirical loss minimization problem in Equation \eqref{eqn:emloss}. The study of the bounds for the third and fourth terms is referred to as the generalization error analysis of neural networks. 
It is an interesting topic to develop deep learning models and algorithms to minimize optimization and generalization errors.
<br><br> -->
 
	
<!-- 	One of  the key targets in  the area of deep learning is to develop algorithms to reduce  $R_{\mathcal{D}}{({\bm{\theta}}_{\mathcal{N}})}$.
Currently, we focus on providing an upper bound of the approximation  error $R_{\mathcal{D}}(\bm{\theta}_{\mathcal{D}})$ for smooth functions, which is crucial to estimate an upper bound of $R_{\mathcal{D}}{({\bm{\theta}}_{\mathcal{N}})}$.   Instead of deriving an approximator to attain  the approximation error bound,  deep learning models and algorithms aim at identifying a solution $\phi(\bm{x};\bm{\theta}_{\mathcal{N}})$ reducing the generalization and optimization errors in Equation \eqref{eqn:gen}.  Solutions minimizing both generalization and optimization errors will lead to a good solution only if we also have a good upper bound estimate of $R_{\mathcal{D}}(\bm{\theta}_{\mathcal{D}})$ as shown in Equation \eqref{eqn:gen}.   Independent of whether our analysis here  leads to a good approximator, which is an interesting topic to pursue,  the  theory here does provide a key ingredient in the error analysis of deep learning models and algorithms.
 -->


<!-- The approximation theory of neural networks is interesting and technical. Unlike traditional tools to approximate functions, neural networks has two key benefits.
<ul>
  <li>The architecture of neural networks is easy but powerful because of compositions. For example, if we use $f_2\circ f_1$ to approximate $f$, we only need to care the values of $f_2$ on $\text{Im}f_1$, which is extremely efficient when $\text{Im}f_1$ only contains finite points. </li>
  <li>Neural networks is flexible. That is, neural networks can perform well even without the latent structures of the target functions.</li>
</ul> -->
<!-- <ol>
    <li> <b> Approximation Theory.</b> I am pretty interested in technical constructive approxiamtion problems, such as Wavelet, Neural Network.</li>
    <li> <b>Neural Network.</b> I am quite curious about the expresssive power of neural networks to approximate other functions, like continuous functions.
    And I wonder why neural networks work so well in several tasks, such as classification problems. </li>
</ol>

<div class="addWhiteSpaceOne"></div> -->


<!-- <h2> Other Interests:</h2>
<ol>
    <li> <b>Code.</b> I like several code languages, like Latex, Python.
            <ul>
            <li> <b>Latex.</b> I was astonished by Latex when I used Latex for the first time.
            To understant why Latex can typeset contents much better than word, I dive deeper into
            Latex and  learned to write &ldquo;class&rdquo; (.cls) and &ldquo;style&rdquo; (.sty) by myself.</li>

            <li> <b>Python.</b> I learned Python since the first year of my Ph.D. program. Attracted by its rich extensive libraries, 
            I use it to do lots of interesting things: making dynamic images, dealing with pdf files, implementing all kinds of deep learning algorithms, etc.   </li>
            </ul>
    </li><br />
    <li> <b>Dynamic Images.</b> </li>  Dynamic images is interesting and I prefer to make mathematical dynamic images by myself.
    See <a href="../more/"> <b> More</b></a> for examples.
</ol> -->



</section>



<section id="postSection"></section>


<!-- footer section -->
<section>
<footer id="footer"></footer>
</section>





<script type="text/javascript" src="../file/cssAndJs/script.js"></script>
<!-- <script type="text/javascript" src="./interest.js"></script> -->
<script type="text/javascript"> 
    if (phoneOrPc) {var pageWidth=0.84*w;} else {var pageWidth=0.5*w;}

    var insertInterestCss=" .pageContainer {display:block;margin:0 auto;padding-left:3%; padding-right:3%;width:"+pageWidth+"px;}";
    document.getElementById("styleInJs").innerHTML=insertStyle.join('')+insertInterestCss;
        // pre and post section
    var preSection=document.getElementById("preSection");
    var postSection=document.getElementById("postSection");

    if (phoneOrPc){
        preSection.style.height=0.063*h +"px";
        postSection.style.height=0.12*h +"px";
    }else{
        preSection.style.height=0.07*h +"px";
        postSection.style.height=0.12*h +"px";
    }
</script>


<!-- add background on canvas -->
<script type="text/javascript" src="../file/background/bg.js"></script>
</body>
</html>


<!-- 页面 A 跳到 B 页面的特定位置
<a href="../more/#content">点击转到内容</a>
<a href="https://www.baidu.com/#content">点击转到内容</a>
<a id="content">内容部分</a> -->